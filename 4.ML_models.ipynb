{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6VXPfRp-tARR",
    "outputId": "6c27acaf-2c6a-467f-fd7a-68efce98e30b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine # database connection\n",
    "import csv\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.classification import accuracy_score, log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import StratifiedKFold \n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import math\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZihvUPvHtARd"
   },
   "source": [
    "<h1>4. Machine Learning Models </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CtN9VBPutARf"
   },
   "source": [
    "<h2> 4.1 Reading data from file and storing into sql table </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "owBQdjY1tARh"
   },
   "outputs": [],
   "source": [
    "#Creating db file from csv\n",
    "if not os.path.isfile('train.db'):\n",
    "    disk_engine = create_engine('sqlite:///train.db')\n",
    "    start = dt.datetime.now()\n",
    "    chunksize = 180000\n",
    "    j = 0\n",
    "    index_start = 1\n",
    "    for df in pd.read_csv('final_features.csv', names=['Unnamed: 0','id','is_duplicate','cwc_min','cwc_max','csc_min','csc_max','ctc_min','ctc_max','last_word_eq','first_word_eq','abs_len_diff','mean_len','token_set_ratio','token_sort_ratio','fuzz_ratio','fuzz_partial_ratio','longest_substr_ratio','freq_qid1','freq_qid2','q1len','q2len','q1_n_words','q2_n_words','word_Common','word_Total','word_share','freq_q1+q2','freq_q1-q2','0_x','1_x','2_x','3_x','4_x','5_x','6_x','7_x','8_x','9_x','10_x','11_x','12_x','13_x','14_x','15_x','16_x','17_x','18_x','19_x','20_x','21_x','22_x','23_x','24_x','25_x','26_x','27_x','28_x','29_x','30_x','31_x','32_x','33_x','34_x','35_x','36_x','37_x','38_x','39_x','40_x','41_x','42_x','43_x','44_x','45_x','46_x','47_x','48_x','49_x','50_x','51_x','52_x','53_x','54_x','55_x','56_x','57_x','58_x','59_x','60_x','61_x','62_x','63_x','64_x','65_x','66_x','67_x','68_x','69_x','70_x','71_x','72_x','73_x','74_x','75_x','76_x','77_x','78_x','79_x','80_x','81_x','82_x','83_x','84_x','85_x','86_x','87_x','88_x','89_x','90_x','91_x','92_x','93_x','94_x','95_x','96_x','97_x','98_x','99_x','100_x','101_x','102_x','103_x','104_x','105_x','106_x','107_x','108_x','109_x','110_x','111_x','112_x','113_x','114_x','115_x','116_x','117_x','118_x','119_x','120_x','121_x','122_x','123_x','124_x','125_x','126_x','127_x','128_x','129_x','130_x','131_x','132_x','133_x','134_x','135_x','136_x','137_x','138_x','139_x','140_x','141_x','142_x','143_x','144_x','145_x','146_x','147_x','148_x','149_x','150_x','151_x','152_x','153_x','154_x','155_x','156_x','157_x','158_x','159_x','160_x','161_x','162_x','163_x','164_x','165_x','166_x','167_x','168_x','169_x','170_x','171_x','172_x','173_x','174_x','175_x','176_x','177_x','178_x','179_x','180_x','181_x','182_x','183_x','184_x','185_x','186_x','187_x','188_x','189_x','190_x','191_x','192_x','193_x','194_x','195_x','196_x','197_x','198_x','199_x','200_x','201_x','202_x','203_x','204_x','205_x','206_x','207_x','208_x','209_x','210_x','211_x','212_x','213_x','214_x','215_x','216_x','217_x','218_x','219_x','220_x','221_x','222_x','223_x','224_x','225_x','226_x','227_x','228_x','229_x','230_x','231_x','232_x','233_x','234_x','235_x','236_x','237_x','238_x','239_x','240_x','241_x','242_x','243_x','244_x','245_x','246_x','247_x','248_x','249_x','250_x','251_x','252_x','253_x','254_x','255_x','256_x','257_x','258_x','259_x','260_x','261_x','262_x','263_x','264_x','265_x','266_x','267_x','268_x','269_x','270_x','271_x','272_x','273_x','274_x','275_x','276_x','277_x','278_x','279_x','280_x','281_x','282_x','283_x','284_x','285_x','286_x','287_x','288_x','289_x','290_x','291_x','292_x','293_x','294_x','295_x','296_x','297_x','298_x','299_x','300_x','301_x','302_x','303_x','304_x','305_x','306_x','307_x','308_x','309_x','310_x','311_x','312_x','313_x','314_x','315_x','316_x','317_x','318_x','319_x','320_x','321_x','322_x','323_x','324_x','325_x','326_x','327_x','328_x','329_x','330_x','331_x','332_x','333_x','334_x','335_x','336_x','337_x','338_x','339_x','340_x','341_x','342_x','343_x','344_x','345_x','346_x','347_x','348_x','349_x','350_x','351_x','352_x','353_x','354_x','355_x','356_x','357_x','358_x','359_x','360_x','361_x','362_x','363_x','364_x','365_x','366_x','367_x','368_x','369_x','370_x','371_x','372_x','373_x','374_x','375_x','376_x','377_x','378_x','379_x','380_x','381_x','382_x','383_x','0_y','1_y','2_y','3_y','4_y','5_y','6_y','7_y','8_y','9_y','10_y','11_y','12_y','13_y','14_y','15_y','16_y','17_y','18_y','19_y','20_y','21_y','22_y','23_y','24_y','25_y','26_y','27_y','28_y','29_y','30_y','31_y','32_y','33_y','34_y','35_y','36_y','37_y','38_y','39_y','40_y','41_y','42_y','43_y','44_y','45_y','46_y','47_y','48_y','49_y','50_y','51_y','52_y','53_y','54_y','55_y','56_y','57_y','58_y','59_y','60_y','61_y','62_y','63_y','64_y','65_y','66_y','67_y','68_y','69_y','70_y','71_y','72_y','73_y','74_y','75_y','76_y','77_y','78_y','79_y','80_y','81_y','82_y','83_y','84_y','85_y','86_y','87_y','88_y','89_y','90_y','91_y','92_y','93_y','94_y','95_y','96_y','97_y','98_y','99_y','100_y','101_y','102_y','103_y','104_y','105_y','106_y','107_y','108_y','109_y','110_y','111_y','112_y','113_y','114_y','115_y','116_y','117_y','118_y','119_y','120_y','121_y','122_y','123_y','124_y','125_y','126_y','127_y','128_y','129_y','130_y','131_y','132_y','133_y','134_y','135_y','136_y','137_y','138_y','139_y','140_y','141_y','142_y','143_y','144_y','145_y','146_y','147_y','148_y','149_y','150_y','151_y','152_y','153_y','154_y','155_y','156_y','157_y','158_y','159_y','160_y','161_y','162_y','163_y','164_y','165_y','166_y','167_y','168_y','169_y','170_y','171_y','172_y','173_y','174_y','175_y','176_y','177_y','178_y','179_y','180_y','181_y','182_y','183_y','184_y','185_y','186_y','187_y','188_y','189_y','190_y','191_y','192_y','193_y','194_y','195_y','196_y','197_y','198_y','199_y','200_y','201_y','202_y','203_y','204_y','205_y','206_y','207_y','208_y','209_y','210_y','211_y','212_y','213_y','214_y','215_y','216_y','217_y','218_y','219_y','220_y','221_y','222_y','223_y','224_y','225_y','226_y','227_y','228_y','229_y','230_y','231_y','232_y','233_y','234_y','235_y','236_y','237_y','238_y','239_y','240_y','241_y','242_y','243_y','244_y','245_y','246_y','247_y','248_y','249_y','250_y','251_y','252_y','253_y','254_y','255_y','256_y','257_y','258_y','259_y','260_y','261_y','262_y','263_y','264_y','265_y','266_y','267_y','268_y','269_y','270_y','271_y','272_y','273_y','274_y','275_y','276_y','277_y','278_y','279_y','280_y','281_y','282_y','283_y','284_y','285_y','286_y','287_y','288_y','289_y','290_y','291_y','292_y','293_y','294_y','295_y','296_y','297_y','298_y','299_y','300_y','301_y','302_y','303_y','304_y','305_y','306_y','307_y','308_y','309_y','310_y','311_y','312_y','313_y','314_y','315_y','316_y','317_y','318_y','319_y','320_y','321_y','322_y','323_y','324_y','325_y','326_y','327_y','328_y','329_y','330_y','331_y','332_y','333_y','334_y','335_y','336_y','337_y','338_y','339_y','340_y','341_y','342_y','343_y','344_y','345_y','346_y','347_y','348_y','349_y','350_y','351_y','352_y','353_y','354_y','355_y','356_y','357_y','358_y','359_y','360_y','361_y','362_y','363_y','364_y','365_y','366_y','367_y','368_y','369_y','370_y','371_y','372_y','373_y','374_y','375_y','376_y','377_y','378_y','379_y','380_y','381_y','382_y','383_y'], chunksize=chunksize, iterator=True, encoding='utf-8', ):\n",
    "        df.index += index_start\n",
    "        j+=1\n",
    "        print('{} rows'.format(j*chunksize))\n",
    "        df.to_sql('data', disk_engine, if_exists='append')\n",
    "        index_start = df.index[-1] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4hpD3aBktARn"
   },
   "outputs": [],
   "source": [
    "#http://www.sqlitetutorial.net/sqlite-python/create-tables/\n",
    "def create_connection(db_file):\n",
    "    \"\"\" create a database connection to the SQLite database\n",
    "        specified by db_file\n",
    "    :param db_file: database file\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(e)\n",
    " \n",
    "    return None\n",
    "\n",
    "\n",
    "def checkTableExists(dbcon):\n",
    "    cursr = dbcon.cursor()\n",
    "    str = \"select name from sqlite_master where type='table'\"\n",
    "    table_names = cursr.execute(str)\n",
    "    print(\"Tables in the databse:\")\n",
    "    tables =table_names.fetchall() \n",
    "    print(tables[0][0])\n",
    "    return(len(tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nR8ZIUnttARs",
    "outputId": "810fb3fb-7da2-4b78-9e29-9edabbf68cf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the databse:\n",
      "data\n"
     ]
    }
   ],
   "source": [
    "read_db = 'train.db'\n",
    "conn_r = create_connection(read_db)\n",
    "checkTableExists(conn_r)\n",
    "conn_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZq5gaaztARy"
   },
   "outputs": [],
   "source": [
    "# try to sample data according to the computing power you have\n",
    "if os.path.isfile(read_db):\n",
    "    conn_r = create_connection(read_db)\n",
    "    if conn_r is not None:\n",
    "        # for selecting first 1M rows\n",
    "        # data = pd.read_sql_query(\"\"\"SELECT * FROM data LIMIT 100001;\"\"\", conn_r)\n",
    "        \n",
    "        # for selecting random points\n",
    "        data = pd.read_sql_query(\"SELECT * From data ORDER BY RANDOM() LIMIT 100001;\", conn_r)\n",
    "        conn_r.commit()\n",
    "        conn_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZkeBKktKtAR3"
   },
   "outputs": [],
   "source": [
    "# remove the first row \n",
    "data.drop(data.index[0], inplace=True)\n",
    "y_true = data['is_duplicate']\n",
    "data.drop(['Unnamed: 0', 'id','index','is_duplicate'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QKSenpsmtAR9",
    "outputId": "81d890ce-df79-4402-9324-84817dbd5a7d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>csc_min</th>\n",
       "      <th>csc_max</th>\n",
       "      <th>ctc_min</th>\n",
       "      <th>ctc_max</th>\n",
       "      <th>last_word_eq</th>\n",
       "      <th>first_word_eq</th>\n",
       "      <th>abs_len_diff</th>\n",
       "      <th>mean_len</th>\n",
       "      <th>...</th>\n",
       "      <th>374_y</th>\n",
       "      <th>375_y</th>\n",
       "      <th>376_y</th>\n",
       "      <th>377_y</th>\n",
       "      <th>378_y</th>\n",
       "      <th>379_y</th>\n",
       "      <th>380_y</th>\n",
       "      <th>381_y</th>\n",
       "      <th>382_y</th>\n",
       "      <th>383_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166663888935184</td>\n",
       "      <td>0.14285510206997</td>\n",
       "      <td>0.999966667777741</td>\n",
       "      <td>0.333329629670781</td>\n",
       "      <td>0.39999600004</td>\n",
       "      <td>0.22222098766118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.5383271547034</td>\n",
       "      <td>4.86271405220032</td>\n",
       "      <td>25.1317247375846</td>\n",
       "      <td>-12.1952870488167</td>\n",
       "      <td>-5.12827724963427</td>\n",
       "      <td>-1.97450536489487</td>\n",
       "      <td>-10.089723110199</td>\n",
       "      <td>11.8141875807196</td>\n",
       "      <td>-5.10751797258854</td>\n",
       "      <td>-6.89621132612228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999966667777741</td>\n",
       "      <td>0.999966667777741</td>\n",
       "      <td>0.999950002499875</td>\n",
       "      <td>0.66664444518516</td>\n",
       "      <td>0.833319444675922</td>\n",
       "      <td>0.714275510349852</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>...</td>\n",
       "      <td>4.46813777089119</td>\n",
       "      <td>-1.7306991815567</td>\n",
       "      <td>-0.499908775091171</td>\n",
       "      <td>0.284654021263123</td>\n",
       "      <td>-10.1150350123644</td>\n",
       "      <td>-0.559806067496538</td>\n",
       "      <td>-3.90896541625261</td>\n",
       "      <td>9.32843448821222</td>\n",
       "      <td>12.2733947038651</td>\n",
       "      <td>-5.52434960193932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.499987500312492</td>\n",
       "      <td>0.399992000159997</td>\n",
       "      <td>0.222219753113854</td>\n",
       "      <td>0.19999800002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>...</td>\n",
       "      <td>23.0663404017687</td>\n",
       "      <td>16.9184326827526</td>\n",
       "      <td>11.1851321905851</td>\n",
       "      <td>-11.9652707953937</td>\n",
       "      <td>26.5292612314224</td>\n",
       "      <td>16.4487851411104</td>\n",
       "      <td>4.06692604348064</td>\n",
       "      <td>21.5317152440548</td>\n",
       "      <td>5.75854183733463</td>\n",
       "      <td>13.237563662231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999975000624984</td>\n",
       "      <td>0.999975000624984</td>\n",
       "      <td>0.66664444518516</td>\n",
       "      <td>0.66664444518516</td>\n",
       "      <td>0.857130612419823</td>\n",
       "      <td>0.857130612419823</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.1173809766769</td>\n",
       "      <td>3.820263100788</td>\n",
       "      <td>-0.0652444958686829</td>\n",
       "      <td>-7.02229641377926</td>\n",
       "      <td>1.43701577186584</td>\n",
       "      <td>-1.1733054369688</td>\n",
       "      <td>-6.9366245046258</td>\n",
       "      <td>-5.06501710414887</td>\n",
       "      <td>11.7296299934387</td>\n",
       "      <td>8.43634222447872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.333327777870369</td>\n",
       "      <td>0.19999800002</td>\n",
       "      <td>0.666655555740738</td>\n",
       "      <td>0.39999600004</td>\n",
       "      <td>0.499995833368055</td>\n",
       "      <td>0.249998958337674</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.1603710651398</td>\n",
       "      <td>17.7247330965474</td>\n",
       "      <td>10.8354105427861</td>\n",
       "      <td>-5.86775561701506</td>\n",
       "      <td>20.7910571694374</td>\n",
       "      <td>0.237620010972023</td>\n",
       "      <td>-21.7907287180424</td>\n",
       "      <td>-3.2802597284317</td>\n",
       "      <td>19.5695277452469</td>\n",
       "      <td>6.70061165839434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 794 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             cwc_min            cwc_max            csc_min            csc_max  \\\n",
       "1  0.166663888935184   0.14285510206997  0.999966667777741  0.333329629670781   \n",
       "2  0.999966667777741  0.999966667777741  0.999950002499875   0.66664444518516   \n",
       "3                0.0                0.0  0.499987500312492  0.399992000159997   \n",
       "4  0.999975000624984  0.999975000624984   0.66664444518516   0.66664444518516   \n",
       "5  0.333327777870369      0.19999800002  0.666655555740738      0.39999600004   \n",
       "\n",
       "             ctc_min            ctc_max last_word_eq first_word_eq  \\\n",
       "1      0.39999600004   0.22222098766118          0.0           0.0   \n",
       "2  0.833319444675922  0.714275510349852          1.0           0.0   \n",
       "3  0.222219753113854      0.19999800002          0.0           0.0   \n",
       "4  0.857130612419823  0.857130612419823          1.0           0.0   \n",
       "5  0.499995833368055  0.249998958337674          0.0           1.0   \n",
       "\n",
       "  abs_len_diff mean_len        ...                      374_y  \\\n",
       "1          8.0     14.0        ...          -11.5383271547034   \n",
       "2          1.0      6.5        ...           4.46813777089119   \n",
       "3          1.0      9.5        ...           23.0663404017687   \n",
       "4          0.0      7.0        ...           11.1173809766769   \n",
       "5         12.0     18.0        ...           16.1603710651398   \n",
       "\n",
       "              375_y                376_y              377_y  \\\n",
       "1  4.86271405220032     25.1317247375846  -12.1952870488167   \n",
       "2  -1.7306991815567   -0.499908775091171  0.284654021263123   \n",
       "3  16.9184326827526     11.1851321905851  -11.9652707953937   \n",
       "4    3.820263100788  -0.0652444958686829  -7.02229641377926   \n",
       "5  17.7247330965474     10.8354105427861  -5.86775561701506   \n",
       "\n",
       "               378_y               379_y              380_y  \\\n",
       "1  -5.12827724963427   -1.97450536489487   -10.089723110199   \n",
       "2  -10.1150350123644  -0.559806067496538  -3.90896541625261   \n",
       "3   26.5292612314224    16.4487851411104   4.06692604348064   \n",
       "4   1.43701577186584    -1.1733054369688   -6.9366245046258   \n",
       "5   20.7910571694374   0.237620010972023  -21.7907287180424   \n",
       "\n",
       "               381_y              382_y              383_y  \n",
       "1   11.8141875807196  -5.10751797258854  -6.89621132612228  \n",
       "2   9.32843448821222   12.2733947038651  -5.52434960193932  \n",
       "3   21.5317152440548   5.75854183733463    13.237563662231  \n",
       "4  -5.06501710414887   11.7296299934387   8.43634222447872  \n",
       "5   -3.2802597284317   19.5695277452469   6.70061165839434  \n",
       "\n",
       "[5 rows x 794 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KaWHDzqUtASD"
   },
   "source": [
    "<h2> 4.2 Converting strings to numerics </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iLV60gkptASD",
    "outputId": "f297e0f4-52d5-4ab4-8a43-f0ff82f63698"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwc_min\n",
      "cwc_max\n",
      "csc_min\n",
      "csc_max\n",
      "ctc_min\n",
      "ctc_max\n",
      "last_word_eq\n",
      "first_word_eq\n",
      "abs_len_diff\n",
      "mean_len\n",
      "token_set_ratio\n",
      "token_sort_ratio\n",
      "fuzz_ratio\n",
      "fuzz_partial_ratio\n",
      "longest_substr_ratio\n",
      "freq_qid1\n",
      "freq_qid2\n",
      "q1len\n",
      "q2len\n",
      "q1_n_words\n",
      "q2_n_words\n",
      "word_Common\n",
      "word_Total\n",
      "word_share\n",
      "freq_q1+q2\n",
      "freq_q1-q2\n",
      "0_x\n",
      "1_x\n",
      "2_x\n",
      "3_x\n",
      "4_x\n",
      "5_x\n",
      "6_x\n",
      "7_x\n",
      "8_x\n",
      "9_x\n",
      "10_x\n",
      "11_x\n",
      "12_x\n",
      "13_x\n",
      "14_x\n",
      "15_x\n",
      "16_x\n",
      "17_x\n",
      "18_x\n",
      "19_x\n",
      "20_x\n",
      "21_x\n",
      "22_x\n",
      "23_x\n",
      "24_x\n",
      "25_x\n",
      "26_x\n",
      "27_x\n",
      "28_x\n",
      "29_x\n",
      "30_x\n",
      "31_x\n",
      "32_x\n",
      "33_x\n",
      "34_x\n",
      "35_x\n",
      "36_x\n",
      "37_x\n",
      "38_x\n",
      "39_x\n",
      "40_x\n",
      "41_x\n",
      "42_x\n",
      "43_x\n",
      "44_x\n",
      "45_x\n",
      "46_x\n",
      "47_x\n",
      "48_x\n",
      "49_x\n",
      "50_x\n",
      "51_x\n",
      "52_x\n",
      "53_x\n",
      "54_x\n",
      "55_x\n",
      "56_x\n",
      "57_x\n",
      "58_x\n",
      "59_x\n",
      "60_x\n",
      "61_x\n",
      "62_x\n",
      "63_x\n",
      "64_x\n",
      "65_x\n",
      "66_x\n",
      "67_x\n",
      "68_x\n",
      "69_x\n",
      "70_x\n",
      "71_x\n",
      "72_x\n",
      "73_x\n",
      "74_x\n",
      "75_x\n",
      "76_x\n",
      "77_x\n",
      "78_x\n",
      "79_x\n",
      "80_x\n",
      "81_x\n",
      "82_x\n",
      "83_x\n",
      "84_x\n",
      "85_x\n",
      "86_x\n",
      "87_x\n",
      "88_x\n",
      "89_x\n",
      "90_x\n",
      "91_x\n",
      "92_x\n",
      "93_x\n",
      "94_x\n",
      "95_x\n",
      "96_x\n",
      "97_x\n",
      "98_x\n",
      "99_x\n",
      "100_x\n",
      "101_x\n",
      "102_x\n",
      "103_x\n",
      "104_x\n",
      "105_x\n",
      "106_x\n",
      "107_x\n",
      "108_x\n",
      "109_x\n",
      "110_x\n",
      "111_x\n",
      "112_x\n",
      "113_x\n",
      "114_x\n",
      "115_x\n",
      "116_x\n",
      "117_x\n",
      "118_x\n",
      "119_x\n",
      "120_x\n",
      "121_x\n",
      "122_x\n",
      "123_x\n",
      "124_x\n",
      "125_x\n",
      "126_x\n",
      "127_x\n",
      "128_x\n",
      "129_x\n",
      "130_x\n",
      "131_x\n",
      "132_x\n",
      "133_x\n",
      "134_x\n",
      "135_x\n",
      "136_x\n",
      "137_x\n",
      "138_x\n",
      "139_x\n",
      "140_x\n",
      "141_x\n",
      "142_x\n",
      "143_x\n",
      "144_x\n",
      "145_x\n",
      "146_x\n",
      "147_x\n",
      "148_x\n",
      "149_x\n",
      "150_x\n",
      "151_x\n",
      "152_x\n",
      "153_x\n",
      "154_x\n",
      "155_x\n",
      "156_x\n",
      "157_x\n",
      "158_x\n",
      "159_x\n",
      "160_x\n",
      "161_x\n",
      "162_x\n",
      "163_x\n",
      "164_x\n",
      "165_x\n",
      "166_x\n",
      "167_x\n",
      "168_x\n",
      "169_x\n",
      "170_x\n",
      "171_x\n",
      "172_x\n",
      "173_x\n",
      "174_x\n",
      "175_x\n",
      "176_x\n",
      "177_x\n",
      "178_x\n",
      "179_x\n",
      "180_x\n",
      "181_x\n",
      "182_x\n",
      "183_x\n",
      "184_x\n",
      "185_x\n",
      "186_x\n",
      "187_x\n",
      "188_x\n",
      "189_x\n",
      "190_x\n",
      "191_x\n",
      "192_x\n",
      "193_x\n",
      "194_x\n",
      "195_x\n",
      "196_x\n",
      "197_x\n",
      "198_x\n",
      "199_x\n",
      "200_x\n",
      "201_x\n",
      "202_x\n",
      "203_x\n",
      "204_x\n",
      "205_x\n",
      "206_x\n",
      "207_x\n",
      "208_x\n",
      "209_x\n",
      "210_x\n",
      "211_x\n",
      "212_x\n",
      "213_x\n",
      "214_x\n",
      "215_x\n",
      "216_x\n",
      "217_x\n",
      "218_x\n",
      "219_x\n",
      "220_x\n",
      "221_x\n",
      "222_x\n",
      "223_x\n",
      "224_x\n",
      "225_x\n",
      "226_x\n",
      "227_x\n",
      "228_x\n",
      "229_x\n",
      "230_x\n",
      "231_x\n",
      "232_x\n",
      "233_x\n",
      "234_x\n",
      "235_x\n",
      "236_x\n",
      "237_x\n",
      "238_x\n",
      "239_x\n",
      "240_x\n",
      "241_x\n",
      "242_x\n",
      "243_x\n",
      "244_x\n",
      "245_x\n",
      "246_x\n",
      "247_x\n",
      "248_x\n",
      "249_x\n",
      "250_x\n",
      "251_x\n",
      "252_x\n",
      "253_x\n",
      "254_x\n",
      "255_x\n",
      "256_x\n",
      "257_x\n",
      "258_x\n",
      "259_x\n",
      "260_x\n",
      "261_x\n",
      "262_x\n",
      "263_x\n",
      "264_x\n",
      "265_x\n",
      "266_x\n",
      "267_x\n",
      "268_x\n",
      "269_x\n",
      "270_x\n",
      "271_x\n",
      "272_x\n",
      "273_x\n",
      "274_x\n",
      "275_x\n",
      "276_x\n",
      "277_x\n",
      "278_x\n",
      "279_x\n",
      "280_x\n",
      "281_x\n",
      "282_x\n",
      "283_x\n",
      "284_x\n",
      "285_x\n",
      "286_x\n",
      "287_x\n",
      "288_x\n",
      "289_x\n",
      "290_x\n",
      "291_x\n",
      "292_x\n",
      "293_x\n",
      "294_x\n",
      "295_x\n",
      "296_x\n",
      "297_x\n",
      "298_x\n",
      "299_x\n",
      "300_x\n",
      "301_x\n",
      "302_x\n",
      "303_x\n",
      "304_x\n",
      "305_x\n",
      "306_x\n",
      "307_x\n",
      "308_x\n",
      "309_x\n",
      "310_x\n",
      "311_x\n",
      "312_x\n",
      "313_x\n",
      "314_x\n",
      "315_x\n",
      "316_x\n",
      "317_x\n",
      "318_x\n",
      "319_x\n",
      "320_x\n",
      "321_x\n",
      "322_x\n",
      "323_x\n",
      "324_x\n",
      "325_x\n",
      "326_x\n",
      "327_x\n",
      "328_x\n",
      "329_x\n",
      "330_x\n",
      "331_x\n",
      "332_x\n",
      "333_x\n",
      "334_x\n",
      "335_x\n",
      "336_x\n",
      "337_x\n",
      "338_x\n",
      "339_x\n",
      "340_x\n",
      "341_x\n",
      "342_x\n",
      "343_x\n",
      "344_x\n",
      "345_x\n",
      "346_x\n",
      "347_x\n",
      "348_x\n",
      "349_x\n",
      "350_x\n",
      "351_x\n",
      "352_x\n",
      "353_x\n",
      "354_x\n",
      "355_x\n",
      "356_x\n",
      "357_x\n",
      "358_x\n",
      "359_x\n",
      "360_x\n",
      "361_x\n",
      "362_x\n",
      "363_x\n",
      "364_x\n",
      "365_x\n",
      "366_x\n",
      "367_x\n",
      "368_x\n",
      "369_x\n",
      "370_x\n",
      "371_x\n",
      "372_x\n",
      "373_x\n",
      "374_x\n",
      "375_x\n",
      "376_x\n",
      "377_x\n",
      "378_x\n",
      "379_x\n",
      "380_x\n",
      "381_x\n",
      "382_x\n",
      "383_x\n",
      "0_y\n",
      "1_y\n",
      "2_y\n",
      "3_y\n",
      "4_y\n",
      "5_y\n",
      "6_y\n",
      "7_y\n",
      "8_y\n",
      "9_y\n",
      "10_y\n",
      "11_y\n",
      "12_y\n",
      "13_y\n",
      "14_y\n",
      "15_y\n",
      "16_y\n",
      "17_y\n",
      "18_y\n",
      "19_y\n",
      "20_y\n",
      "21_y\n",
      "22_y\n",
      "23_y\n",
      "24_y\n",
      "25_y\n",
      "26_y\n",
      "27_y\n",
      "28_y\n",
      "29_y\n",
      "30_y\n"
     ]
    }
   ],
   "source": [
    "# after we read from sql table each entry was read it as a string\n",
    "# we convert all the features into numaric before we apply any model\n",
    "cols = list(data.columns)\n",
    "for i in cols:\n",
    "    data[i] = data[i].apply(pd.to_numeric)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_LpfQwc9tASJ"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/7368789/convert-all-strings-in-a-list-to-int\n",
    "y_true = list(map(int, y_true.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CuMTqWGutASO"
   },
   "source": [
    "<h2> 4.3 Random train test split( 70:30) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Rat2obGtASP"
   },
   "outputs": [],
   "source": [
    "X_train,X_test, y_train, y_test = train_test_split(data, y_true, stratify=y_true, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Iw9zCHqtASS",
    "outputId": "910b684b-0876-4dd8-e0d9-457846236833"
   },
   "outputs": [],
   "source": [
    "print(\"Number of data points in train data :\",X_train.shape)\n",
    "print(\"Number of data points in test data :\",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0oDV15LJtASY",
    "outputId": "70a1e4eb-3f31-4f1e-a53b-ad972978505d"
   },
   "outputs": [],
   "source": [
    "print(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\n",
    "train_distr = Counter(y_train)\n",
    "train_len = len(y_train)\n",
    "print(\"Class 0: \",int(train_distr[0])/train_len,\"Class 1: \", int(train_distr[1])/train_len)\n",
    "print(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\n",
    "test_distr = Counter(y_test)\n",
    "test_len = len(y_test)\n",
    "print(\"Class 0: \",int(test_distr[1])/test_len, \"Class 1: \",int(test_distr[1])/test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XfxcPT6jtASg"
   },
   "outputs": [],
   "source": [
    "# This function plots the confusion matrices given y_i, y_i_hat.\n",
    "def plot_confusion_matrix(test_y, predict_y):\n",
    "    C = confusion_matrix(test_y, predict_y)\n",
    "    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n",
    "    \n",
    "    A =(((C.T)/(C.sum(axis=1))).T)\n",
    "    #divid each element of the confusion matrix with the sum of elements in that column\n",
    "    \n",
    "    # C = [[1, 2],\n",
    "    #     [3, 4]]\n",
    "    # C.T = [[1, 3],\n",
    "    #        [2, 4]]\n",
    "    # C.sum(axis = 1)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n",
    "    # C.sum(axix =1) = [[3, 7]]\n",
    "    # ((C.T)/(C.sum(axis=1))) = [[1/3, 3/7]\n",
    "    #                           [2/3, 4/7]]\n",
    "\n",
    "    # ((C.T)/(C.sum(axis=1))).T = [[1/3, 2/3]\n",
    "    #                           [3/7, 4/7]]\n",
    "    # sum of row elements = 1\n",
    "    \n",
    "    B =(C/C.sum(axis=0))\n",
    "    #divid each element of the confusion matrix with the sum of elements in that row\n",
    "    # C = [[1, 2],\n",
    "    #     [3, 4]]\n",
    "    # C.sum(axis = 0)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n",
    "    # C.sum(axix =0) = [[4, 6]]\n",
    "    # (C/C.sum(axis=0)) = [[1/4, 2/6],\n",
    "    #                      [3/4, 4/6]] \n",
    "    plt.figure(figsize=(20,4))\n",
    "    \n",
    "    labels = [1,2]\n",
    "    # representing A in heatmap format\n",
    "    cmap=sns.light_palette(\"blue\")\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Precision matrix\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    # representing B in heatmap format\n",
    "    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Recall matrix\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UStQJ5F_tASk"
   },
   "source": [
    "<h2> 4.4 Building a random model (Finding worst-case log-loss) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qwMDqcU7tASl",
    "outputId": "c1e90d53-25ec-445b-e33a-299538520e32"
   },
   "outputs": [],
   "source": [
    "# we need to generate 9 numbers and the sum of numbers should be 1\n",
    "# one solution is to genarate 9 numbers and divide each of the numbers by their sum\n",
    "# ref: https://stackoverflow.com/a/18662466/4084039\n",
    "# we create a output array that has exactly same size as the CV data\n",
    "predicted_y = np.zeros((test_len,2))\n",
    "for i in range(test_len):\n",
    "    rand_probs = np.random.rand(1,2)\n",
    "    predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\n",
    "print(\"Log loss on Test Data using Random Model\",log_loss(y_test, predicted_y, eps=1e-15))\n",
    "\n",
    "predicted_y =np.argmax(predicted_y, axis=1)\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YgY29g_qtASq"
   },
   "source": [
    "<h2> 4.4 Logistic Regression with hyperparameter tuning </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wb2tOE3GtASr",
    "outputId": "d7e4fc88-7d4e-4313-cda7-462a2409292e"
   },
   "outputs": [],
   "source": [
    "alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
    "\n",
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: \n",
    "#------------------------------\n",
    "\n",
    "\n",
    "log_error_array=[]\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(X_train, y_train)\n",
    "    predict_y = sig_clf.predict_proba(X_test)\n",
    "    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(X_train, y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(X_train)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(X_test)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predicted_y =np.argmax(predict_y,axis=1)\n",
    "print(\"Total number of data points :\", len(predicted_y))\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ouQSEnr3tASy"
   },
   "source": [
    "<h2> 4.5 Linear SVM with hyperparameter tuning </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOFfZ5PLtAS0",
    "outputId": "d31eb598-e275-48cb-c49b-98e9eb76d8ba"
   },
   "outputs": [],
   "source": [
    "alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
    "\n",
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: \n",
    "#------------------------------\n",
    "\n",
    "\n",
    "log_error_array=[]\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l1', loss='hinge', random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(X_train, y_train)\n",
    "    predict_y = sig_clf.predict_proba(X_test)\n",
    "    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l1', loss='hinge', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(X_train, y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(X_train)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(X_test)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predicted_y =np.argmax(predict_y,axis=1)\n",
    "print(\"Total number of data points :\", len(predicted_y))\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZhTJgclztAS6"
   },
   "source": [
    "<h2> 4.6 XGBoost </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9U367-xetAS7",
    "outputId": "167e8588-2ac4-4c6d-ac22-f56a2fce5657"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 4\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_test, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=20, verbose_eval=10)\n",
    "\n",
    "xgdmat = xgb.DMatrix(X_train,y_train)\n",
    "predict_y = bst.predict(d_test)\n",
    "print(\"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6U5b17AatAS_",
    "outputId": "ca83b680-023b-4bc5-f499-8d8d85c2ff5e"
   },
   "outputs": [],
   "source": [
    "predicted_y =np.array(predict_y>0.5,dtype=int)\n",
    "print(\"Total number of data points :\", len(predicted_y))\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WmiIgHOJtATF"
   },
   "source": [
    "<h1> 5. Assignments </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CWS6JoB0tATF"
   },
   "source": [
    "1. Try out models (Logistic regression, Linear-SVM) with simple TF-IDF vectors instead of TD_IDF weighted word2Vec.\n",
    "2. Hyperparameter tune XgBoost using RandomSearch to reduce the log-loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "4.ML_models.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
